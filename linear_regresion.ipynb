{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory path\n",
    "import os\n",
    "import regresion as lr\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join('data', 'Uni_linear.txt'), delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        0        1\n0  6.1101  17.5920\n1  5.5277   9.1302\n2  8.5186  13.6620\n3  7.0032  11.8540\n4  5.8598   6.8233",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>6.1101</td>\n      <td>17.5920</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>5.5277</td>\n      <td>9.1302</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>8.5186</td>\n      <td>13.6620</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>7.0032</td>\n      <td>11.8540</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>5.8598</td>\n      <td>6.8233</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(feature, label, theta):\n",
    "    \"\"\"\n",
    "    Compute cost for linear regression. Computes the cost of using theta as the\n",
    "    parameter for linear regression to fit the data points in feature (X) and label (y).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : array_like\n",
    "        The input dataset of shape (m x n+1), where m is the number of examples,\n",
    "        and n is the number of features. We assume a vector of one's already\n",
    "        appended to the features so we have n+1 columns.\n",
    "\n",
    "    label : array_like\n",
    "        The values of the function at each data point. This is a vector of\n",
    "        shape (m, ).\n",
    "\n",
    "    theta : array_like\n",
    "        The parameters for the regression function. This is a vector of\n",
    "        shape (n+1, ).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cost : float\n",
    "        The value of the regression cost function.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize some useful values\n",
    "    total_examples = len(label)  # number of training examples (m)\n",
    "\n",
    "    predictions = feature.dot(theta)  # Predictions of the hypothesis on all samples\n",
    "\n",
    "    sqr_errors = (predictions - label) ** 2\n",
    "\n",
    "    cost = 1 / (2 * total_examples) * np.sum(sqr_errors)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feature_label = data.values\n",
    "\n",
    "total_examples = data_feature_label[:, 0].size  # number of training examples\n",
    "\n",
    "# Setup Feature and Label.\n",
    "feature = np.append(np.ones((total_examples, 1)), data_feature_label[:, 0].reshape(total_examples, 1), axis=1)\n",
    "label = data_feature_label[:, 1].reshape(total_examples, 1)\n",
    "\n",
    "# Initialize theta to zero values\n",
    "theta = np.zeros((2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cost(feature, label, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(feature, label, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn `theta`. Updates theta by taking `num_iters`\n",
    "    gradient steps with learning rate `alpha`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : array_like\n",
    "        The input dataset of shape (m x n+1).\n",
    "\n",
    "    label : array_like\n",
    "        Value at given features. A vector of shape (m, ).\n",
    "\n",
    "    theta : array_like\n",
    "        Initial values for the linear regression parameters.\n",
    "        A vector of shape (n+1, ).\n",
    "\n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "\n",
    "    num_iters : int\n",
    "        The number of iterations for gradient descent.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    theta : array_like\n",
    "        The learned linear regression parameters. A vector of shape (n+1, ).\n",
    "\n",
    "    cost_history : list\n",
    "        A python list for the values of the cost function after each iteration.\n",
    "\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    total_examples = len(label)  # number of training examples\n",
    "\n",
    "    cost_history = []  # Use a python list to save cost in every iteration\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        predictions = feature.dot(theta)\n",
    "\n",
    "        error = np.dot(feature.transpose(), (predictions - label))\n",
    "\n",
    "        descent = alpha * 1 / total_examples * error\n",
    "\n",
    "        theta -= descent\n",
    "\n",
    "        # save the cost in every iteration\n",
    "        cost_history.append(compute_cost(feature, label, theta))\n",
    "\n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some gradient descent settings\n",
    "iterations = 1500\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, cost_history = lr.gradient_descent(feature, label, theta, alpha, iterations)\n",
    "print('Theta found by gradient descent: ' + str(round(theta[0, 0], 2)) + ' , ' + str(round(theta[1, 0], 2)))\n",
    "print(\"h(x) =\"+str(round(theta[0,0],2))+\" + \"+str(round(theta[1,0],2))+\"x1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(to_predict, theta):\n",
    "    \"\"\"\n",
    "    Takes in numpy array of to_predict and theta and return the predicted value of y based on theta\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = np.dot(theta.transpose(), to_predict)\n",
    "\n",
    "    return predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values for population sizes of 35,000 and 70,000\n",
    "predict1 = predict(np.array([1, 3.5]), theta) * 10000\n",
    "print('For population = 35,000, we predict a profit of : ' + str(round(predict1, 2)))\n",
    "\n",
    "predict2 = predict(np.array([1, 7]), theta) * 10000\n",
    "print('For population = 70,000, we predict a profit of : ' + str(round(predict2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid over which we will calculate J\n",
    "theta0_vals = np.linspace(-10, 10, 100)\n",
    "theta1_vals = np.linspace(-1, 4, 100)\n",
    "\n",
    "# initialize J_vals to a matrix of 0's\n",
    "J_vals = np.zeros((theta0_vals.shape[0], theta1_vals.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out J_vals\n",
    "for i, theta0 in enumerate(theta0_vals):\n",
    "    for j, theta1 in enumerate(theta1_vals):\n",
    "        J_vals[i, j] = compute_cost(feature, label, [theta0, theta1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerate(theta0_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of the way meshgrids work in the surf command, we need to\n",
    "# transpose J_vals before calling surf, or else the axes will be flipped\n",
    "J_vals = J_vals.T\n",
    "J_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# surface plot\n",
    "fig = pyplot.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap='viridis')\n",
    "pyplot.xlabel('theta0')\n",
    "pyplot.ylabel('theta1')\n",
    "pyplot.title('Surface')\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(os.path.join('data', 'ex1data2.txt'), delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_normalize(feature):\n",
    "    \"\"\"\n",
    "    Normalizes the features in feature. returns a normalized version of feature where\n",
    "    the mean value of each feature is 0 and the standard deviation\n",
    "    is 1. This is often a good preprocessing step to do when working with\n",
    "    learning algorithms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : array_like\n",
    "        The dataset of shape (m x n).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feature_norm : array_like\n",
    "        The normalized dataset of shape (m x n).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    mean = np.mean(feature, axis=0)\n",
    "\n",
    "    std = np.std(feature, axis=0)\n",
    "\n",
    "    feature_norm = (feature - mean) / std\n",
    "\n",
    "    return feature_norm, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_feature_label = data2.values\n",
    "\n",
    "total_examples2 = len(data2_feature_label[:, -1])  # number of training examples\n",
    "\n",
    "# Setup Feature and Label.\n",
    "feature2 = data2_feature_label[:, 0:2].reshape(total_examples2, 2)\n",
    "label2 = data2_feature_label[:, -1].reshape(total_examples2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call featureNormalize on the loaded data\n",
    "feature_norm, mean, sigma = feature_normalize(feature2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_norm = np.append(np.ones((total_examples2, 1)), feature_norm, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize theta to zero values\n",
    "theta2 = np.zeros((3, 1))\n",
    "cost2 = compute_cost(feature_norm, label2, theta2)  # Calculate initial cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some gradient descent settings\n",
    "iterations2 = 1400\n",
    "alpha2 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta2, cost2_history = gradient_descent(feature_norm, label2, theta2, alpha2, iterations2)\n",
    "print(\"h(x) =\"+str(round(theta2[0, 0],2))+\" + \"+str(round(theta2[1, 0],2))+\"x1 + \"+str(round(theta2[2, 0],2))+\"x2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost2_history[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(cost2_history)\n",
    "pyplot.xlabel(\"Iteration\")\n",
    "pyplot.ylabel(\"$J(\\Theta)$\")\n",
    "pyplot.title(\"Cost function using Gradient Descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values using optimized theta values using gradient descent\n",
    "feature_sample = feature_normalize(np.array([1650, 3]))[0]\n",
    "feature_sample = np.append(np.ones(1), feature_sample)\n",
    "predict3 = predict(feature_sample, theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For size of house = 1650, Number of bedroom = 3, we predict a house value of $\"+str(round(predict3,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}